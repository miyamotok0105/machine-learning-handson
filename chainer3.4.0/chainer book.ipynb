{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy.png        chainer book.ipynb  loss.png\r\n",
      "cg.dot              log                 snapshot_iter_600\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy.png       chainer book.ipynb loss.png\r\n",
      "cg.dot             log                snapshot_iter_600\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://play.chainer.org/book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miyamoto/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.]\n",
      "[3. 6.]\n"
     ]
    }
   ],
   "source": [
    "from chainer import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x_data = np.array([5], dtype=np.float32)\n",
    "x = Variable(x_data)\n",
    "y = x ** 2 - 2 * x + 1\n",
    "print(y.data)\n",
    "\n",
    "z_data = np.array([[2, 3, 4], [5, 6, 7]], dtype=np.float32)\n",
    "z = Variable(z_data)\n",
    "print(z[:, 1].data)  # [[3], [6]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.]\n",
      "[ 0.2 -0.2]\n"
     ]
    }
   ],
   "source": [
    "from chainer import Variable\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([5], dtype=np.float32)\n",
    "x = Variable(x_data)\n",
    "y = x ** 2 - 2 * x + 1\n",
    "y.backward()\n",
    "\n",
    "z = Variable(np.array([10, 20], dtype=np.float32))\n",
    "zz = 2 * z\n",
    "zz.grad = np.array([0.1, -0.1], dtype=np.float32)\n",
    "zz.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(z.grad)\n",
    "\n",
    "# print(y.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.]\n",
      "[ 0.4 -0.4]\n"
     ]
    }
   ],
   "source": [
    "#1回目\n",
    "y.backward()\n",
    "zz.backward()\n",
    "print(x.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.]\n",
      "[ 0.6 -0.6]\n"
     ]
    }
   ],
   "source": [
    "#2回目\n",
    "y.backward()\n",
    "zz.backward()\n",
    "print(x.grad)\n",
    "print(z.grad)\n",
    "#gradが変わっていっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.]\n",
      "[ 0.6 -0.6]\n"
     ]
    }
   ],
   "source": [
    "#unchain\n",
    "y.unchain_backward()\n",
    "zz.unchain_backward()\n",
    "print(x.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.]\n",
      "[ 0.6 -0.6]\n"
     ]
    }
   ],
   "source": [
    "#3回目\n",
    "y.backward()\n",
    "zz.backward()\n",
    "print(x.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.]\n",
      "[ 0.6 -0.6]\n"
     ]
    }
   ],
   "source": [
    "#４回目\n",
    "y.backward()\n",
    "zz.backward()\n",
    "print(x.grad)\n",
    "print(z.grad)\n",
    "#chainを切った場所以前の値が変わらない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.]\n"
     ]
    }
   ],
   "source": [
    "print(y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable([2.])\n",
      "variable([17.676245])\n",
      "loss: variable(251.04852)\n",
      "variable([10.])\n",
      "variable([54.024258])\n",
      "loss: variable(1953.6505)\n",
      "variable([-2.])\n",
      "variable([1.7146585])\n",
      "loss: variable(29.334475)\n",
      "variable([6.])\n",
      "variable([43.24403])\n",
      "loss: variable(668.08203)\n",
      "variable([-10.])\n",
      "variable([-39.305027])\n",
      "loss: variable(1.2363309)\n",
      "variable([-8.])\n",
      "variable([-28.850517])\n",
      "loss: variable(166.77892)\n",
      "variable([8.])\n",
      "variable([42.773113])\n",
      "loss: variable(73.25318)\n",
      "variable([-4.])\n",
      "variable([-9.691114])\n",
      "loss: variable(322.9339)\n",
      "variable([-6.])\n",
      "variable([-23.670183])\n",
      "loss: variable(505.24258)\n",
      "variable([4.])\n",
      "variable([25.64906])\n",
      "loss: variable(87.54749)\n",
      "variable([0.])\n",
      "variable([10.642519])\n",
      "loss: variable(77.94192)\n",
      "variable([-2.])\n",
      "variable([1.7146585])\n",
      "loss: variable(319.61743)\n",
      "variable([10.])\n",
      "variable([54.024258])\n",
      "loss: variable(1825.6501)\n",
      "variable([-6.])\n",
      "variable([-23.670183])\n",
      "loss: variable(732.0652)\n",
      "variable([2.])\n",
      "variable([17.676245])\n",
      "loss: variable(1.6032312)\n",
      "variable([0.])\n",
      "variable([10.642519])\n",
      "loss: variable(55.78848)\n",
      "variable([6.])\n",
      "variable([43.24403])\n",
      "loss: variable(4.5641165)\n",
      "variable([-4.])\n",
      "variable([-9.691114])\n",
      "loss: variable(68.11403)\n",
      "variable([-10.])\n",
      "variable([-39.305027])\n",
      "loss: variable(3.016995)\n",
      "variable([4.])\n",
      "variable([25.64906])\n",
      "loss: variable(49.25224)\n",
      "variable([8.])\n",
      "variable([42.773113])\n",
      "loss: variable(279.24063)\n",
      "variable([-8.])\n",
      "variable([-28.850517])\n",
      "loss: variable(309.03467)\n"
     ]
    }
   ],
   "source": [
    "from chainer import Chain\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer import optimizers\n",
    "from chainer import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Linear(Chain):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(1, 1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 5. * x + 10\n",
    "\n",
    "\n",
    "x = np.linspace(-10, 10, num=11)\n",
    "y = f(x) + 5. * np.random.randn(11)\n",
    "\n",
    "model = Linear()\n",
    "#cleargradsしないとnanになったりするので注意！！！\n",
    "model.cleargrads()\n",
    "opt = optimizers.SGD(lr=0.001)\n",
    "opt.setup(model)\n",
    "for epoch in range(2):\n",
    "    perm = np.random.permutation(len(x))\n",
    "    for i in range(len(x)):\n",
    "        x_i = Variable(np.array([[x[perm[i]]]], dtype=np.float32))\n",
    "        y_i = Variable(np.array([[y[perm[i]]]], dtype=np.float32))\n",
    "        print(x_i[0])\n",
    "        print(y_i[0])\n",
    "        _y = model(x_i)\n",
    "        loss = F.mean_squared_error(_y, y_i)\n",
    "        loss.backward()\n",
    "        opt.update()\n",
    "        print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えばsoftmax_cross_entropyはdtypeもfとiになってるので注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[-1, 0, 1, 2], [2, 0, 1, -1]]).astype('f')\n",
    "t = np.array([3, 0]).astype('i')\n",
    "y = F.softmax_cross_entropy(x, t)\n",
    "log_softmax = -F.log_softmax(x)\n",
    "expected_loss = np.mean([log_softmax[row, column].data for row, column in enumerate(t)])\n",
    "y.array == expected_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manually assigned Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/chainer/chainer/issues/3868\n",
    "#https://twitter.com/im132nd/status/871751800847609857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.]\n",
      "None\n",
      "<chainer.variable.VariableNode object at 0x114997c18>\n",
      "float32\n",
      "float32\n",
      "[3. 6.]\n"
     ]
    }
   ],
   "source": [
    "from chainer import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x_data = np.array([5], dtype=np.float32)\n",
    "x = Variable(x_data)\n",
    "y = x ** 2 - 2 * x + 1\n",
    "print(y.data)\n",
    "print(y._grad_var)\n",
    "print(y._node)\n",
    "print(x.node.dtype)\n",
    "print(x.node.data.dtype)\n",
    "\n",
    "z_data = np.array([[2, 3, 4], [5, 6, 7]], dtype=np.float32)\n",
    "z = Variable(z_data)\n",
    "print(z[:, 1].data)  # [[3], [6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "import numpy\n",
    "from chainer import functions as F\n",
    "\n",
    "x = chainer.Variable(numpy.array([3, 2], dtype=numpy.float32))\n",
    "y = F.sin(x)\n",
    "x.data = numpy.array([4, 2], dtype=numpy.float64)\n",
    "\n",
    "print(x.node.dtype)\n",
    "print(x.node.data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shapeとdtypeを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==input==\n",
      "variable([[1. 1. 1. 1. 1.]\n",
      "          [1. 1. 1. 1. 1.]\n",
      "          [1. 1. 1. 1. 1.]])\n",
      "variable([1 1 1])\n",
      "==model call==\n",
      "(3, 2)\n",
      "2\n",
      "(3,)\n",
      "1\n",
      "None\n",
      "loss: 0.6528654\n",
      "x grad: None\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import chainer\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer import Variable\n",
    "from chainer import optimizers\n",
    "import numpy as np\n",
    "\n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
    "    def __call__(self, x, t):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        print(\"==model call==\")\n",
    "        print(y.shape)\n",
    "        print(y.ndim)\n",
    "        print(t.shape)\n",
    "        print(t.ndim)\n",
    "#         print(y.data)\n",
    "        print(y._grad_var)\n",
    "        return F.softmax_cross_entropy(y, t)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return F.softmax(self.l3(h2))\n",
    "\n",
    "model = MLP(5, 2)\n",
    "model.cleargrads()\n",
    "optimizer = optimizers.SGD(lr=0.001)\n",
    "optimizer.setup(model)\n",
    "x = Variable(np.ones((3, 5), dtype=np.float32))\n",
    "t = Variable(np.ones((3), dtype=np.int32))\n",
    "print(\"==input==\")\n",
    "print(x)\n",
    "print(t)\n",
    "loss = model(x, t)\n",
    "print(\"loss:\", loss.data)\n",
    "print(\"x grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradの動きを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==input==\n",
      "variable([[1. 1. 1. 1. 1.]\n",
      "          [1. 1. 1. 1. 1.]\n",
      "          [1. 1. 1. 1. 1.]])\n",
      "variable([1 1 1])\n",
      "loss: 1.4252744\n",
      "x grad: None\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import chainer\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer import Variable\n",
    "from chainer import optimizers\n",
    "import numpy as np\n",
    "\n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
    "    def __call__(self, x, t):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "        return F.softmax_cross_entropy(y, t)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return F.softmax(self.l3(h2))\n",
    "\n",
    "model = MLP(5, 2)\n",
    "model.cleargrads()\n",
    "optimizer = optimizers.SGD(lr=0.001)\n",
    "optimizer.setup(model)\n",
    "x = Variable(np.ones((3, 5), dtype=np.float32))\n",
    "t = Variable(np.ones((3), dtype=np.int32))\n",
    "print(\"==input==\")\n",
    "print(x)\n",
    "print(t)\n",
    "loss = model(x, t)\n",
    "print(\"loss:\", loss.data)\n",
    "print(\"x grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==backward==\n",
      "loss: 1.4252744\n",
      "x grad: [[ 0.14618626 -0.0203914   0.12454231 -0.00858422  0.0494754 ]\n",
      " [ 0.14618626 -0.0203914   0.12454231 -0.00858422  0.0494754 ]\n",
      " [ 0.14618626 -0.0203914   0.12454231 -0.00858422  0.0494754 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"==backward==\")\n",
    "loss.backward()\n",
    "loss = model(x, t)\n",
    "print(\"loss:\", loss.data)\n",
    "print(\"x grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==backward==\n",
      "loss: 1.4252744\n",
      "x grad: [[ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]\n",
      " [ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]\n",
      " [ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"==backward==\")\n",
    "loss.backward()\n",
    "loss = model(x, t)\n",
    "\n",
    "print(\"loss:\", loss.data)\n",
    "print(\"x grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradは変わってるが当然lossは変わらない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重み係数を更新する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4081409\n",
      "x grad: [[ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]\n",
      " [ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]\n",
      " [ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]]\n"
     ]
    }
   ],
   "source": [
    "optimizer.update()\n",
    "loss = model(x, t)\n",
    "print(\"loss:\", loss.data)\n",
    "print(\"x grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3912008\n",
      "x grad: [[ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]\n",
      " [ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]\n",
      " [ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]]\n"
     ]
    }
   ],
   "source": [
    "optimizer.update()\n",
    "loss = model(x, t)\n",
    "print(\"loss:\", loss.data)\n",
    "print(\"x grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3744533\n",
      "x grad: [[ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]\n",
      " [ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]\n",
      " [ 0.29237252 -0.0407828   0.24908462 -0.01716844  0.0989508 ]]\n"
     ]
    }
   ],
   "source": [
    "optimizer.update()\n",
    "loss = model(x, t)\n",
    "print(\"loss:\", loss.data)\n",
    "print(\"x grad:\", x.grad)\n",
    "#gradは変わってないが、係数は更新されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3664272\n",
      "x grad: [[ 0.4311268  -0.06172036  0.3664877  -0.02553374  0.14538176]\n",
      " [ 0.4311268  -0.06172036  0.3664877  -0.02553374  0.14538176]\n",
      " [ 0.4311268  -0.06172036  0.3664877  -0.02553374  0.14538176]]\n"
     ]
    }
   ],
   "source": [
    "optimizer.update(model, x, t)\n",
    "loss = model(x, t)\n",
    "print(\"loss:\", loss.data)\n",
    "print(\"x grad:\", x.grad)\n",
    "#lossとgradと係数に変化がある。\n",
    "#optimizer.update(,,)を使うと内部でbackwardもやる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load_irisを使ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データセットを変える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "[5.1 3.5 1.4 0.2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "Y = iris.target\n",
    "Y = Y.flatten().astype(np.int32)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X[0])\n",
    "print(Y) #4つの特徴量で3つのクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Variable(X)\n",
    "Y = Variable(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]\n",
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==input==\n",
      "loss: 1.8300899\n",
      "x grad: [[ 0.00861229  0.00291787 -0.00866771  0.00900884 -0.00604716]\n",
      " [ 0.00861229  0.00291787 -0.00866771  0.00900884 -0.00604716]\n",
      " [ 0.00861229  0.00291787 -0.00866771  0.00900884 -0.00604716]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import chainer\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer import Variable\n",
    "import numpy as np\n",
    "\n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
    "    def __call__(self, x, t):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        y = self.l3(h2)\n",
    "#         print(y)\n",
    "#         print(y.shape)\n",
    "        return F.softmax_cross_entropy(y, t)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return F.softmax(self.l3(h2))\n",
    "\n",
    "model = MLP(10, 3)\n",
    "model.cleargrads()\n",
    "print(\"==input==\")\n",
    "loss = model(X, Y)\n",
    "print(\"loss:\", loss.data)\n",
    "print(\"x grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: variable([[0.27109733 0.25443485 0.4744678 ]])\n",
      "predict max: [2]\n"
     ]
    }
   ],
   "source": [
    "print(\"predict:\", model.predict(np.array([[1.1,1.5,1.4,0.2]], dtype=np.float32)))\n",
    "print(\"predict max:\", model.predict(np.array([[1.1,1.5,1.4,0.2]], dtype=np.float32)).data.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: variable([[0.22469056 0.03719145 0.73811793]])\n",
      "predict max: [2]\n"
     ]
    }
   ],
   "source": [
    "print(\"predict:\", model.predict(np.array([[5.1,3.5,1.4,0.2]], dtype=np.float32)))\n",
    "print(\"predict max:\", model.predict(np.array([[5.1,3.5,1.4,0.2]], dtype=np.float32)).data.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==backward==\n",
      "loss: 1.2160245\n",
      "x grad: [[ 0.00861229  0.00291787 -0.00866771  0.00900884 -0.00604716]\n",
      " [ 0.00861229  0.00291787 -0.00866771  0.00900884 -0.00604716]\n",
      " [ 0.00861229  0.00291787 -0.00866771  0.00900884 -0.00604716]]\n"
     ]
    }
   ],
   "source": [
    "print(\"==backward==\")\n",
    "#retain_grad=True\n",
    "loss.backward(retain_grad=False)\n",
    "loss = model(X, Y)\n",
    "print(\"loss:\", loss.data)\n",
    "print(\"x grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD(lr=0.00001)\n",
    "optimizer.setup(model)\n",
    "optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.216\n"
     ]
    }
   ],
   "source": [
    "loss = model(X, Y)\n",
    "print(\"loss:\", loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable W([[-0.3016114 ,  0.18870594,  0.1085281 ,  0.118176  ],\n",
       "            [-0.37841696, -0.00885908,  0.56859565,  0.25816032]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.l1.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable b([0.000000e+00, 1.739527e-06])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.l1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: variable(0.06108605)\n"
     ]
    }
   ],
   "source": [
    "model.cleargrads()\n",
    "for i in range(200):\n",
    "    loss = model(X, Y)\n",
    "    loss.backward(retain_grad=False)\n",
    "    optimizer = optimizers.SGD(lr=0.001)\n",
    "    optimizer.setup(model)\n",
    "    optimizer.update()\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: variable([[3.3553846e-08 3.6883622e-04 9.9963117e-01]])\n",
      "predict max: [2]\n"
     ]
    }
   ],
   "source": [
    "print(\"predict:\", model.predict(np.array([[0.1,0.5,0.4,5.2]], dtype=np.float32)))\n",
    "print(\"predict max:\", model.predict(np.array([[0.1,0.5,0.4,5.2]], dtype=np.float32)).data.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: variable([[9.9980384e-01 1.9615908e-04 1.3166370e-21]])\n",
      "predict max: [0]\n"
     ]
    }
   ],
   "source": [
    "print(\"predict:\", model.predict(np.array([[5.1,3.5,1.4,0.2]], dtype=np.float32)))\n",
    "print(\"predict max:\", model.predict(np.array([[5.1,3.5,1.4,0.2]], dtype=np.float32)).data.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "5\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J     total [########..........................................] 16.67%\n",
      "this epoch [########..........................................] 16.67%\n",
      "       100 iter, 0 epoch / 1 epochs\n",
      "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
      "\u001b[4A\u001b[J     total [################..................................] 33.33%\n",
      "this epoch [################..................................] 33.33%\n",
      "       200 iter, 0 epoch / 1 epochs\n",
      "     20.16 iters/sec. Estimated time to finish: 0:00:19.841736.\n",
      "\u001b[4A\u001b[J     total [#########################.........................] 50.00%\n",
      "this epoch [#########################.........................] 50.00%\n",
      "       300 iter, 0 epoch / 1 epochs\n",
      "    20.621 iters/sec. Estimated time to finish: 0:00:14.548076.\n",
      "\u001b[4A\u001b[J     total [#################################.................] 66.67%\n",
      "this epoch [#################################.................] 66.67%\n",
      "       400 iter, 0 epoch / 1 epochs\n",
      "    21.033 iters/sec. Estimated time to finish: 0:00:09.509079.\n",
      "\u001b[4A\u001b[J     total [#########################################.........] 83.33%\n",
      "this epoch [#########################################.........] 83.33%\n",
      "       500 iter, 0 epoch / 1 epochs\n",
      "     21.72 iters/sec. Estimated time to finish: 0:00:04.604089.\n",
      "\u001b[4A\u001b[J1           0.192063    0.100027              0.942033       0.9689                    27.6162       \n",
      "\u001b[J     total [##################################################] 100.00%\n",
      "this epoch [..................................................]  0.00%\n",
      "       600 iter, 1 epoch / 1 epochs\n",
      "    21.145 iters/sec. Estimated time to finish: 0:00:00.\n",
      "\u001b[4A\u001b[J"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer import report, training, Chain, datasets, iterators, optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from chainer.datasets import tuple_dataset\n",
    "\n",
    "\n",
    "\n",
    "# Network definition\n",
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)  # n_units -> n_out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)\n",
    "\n",
    "\n",
    "def main():\n",
    "    unit = 1000\n",
    "    batchsize = 100\n",
    "    gpu = -1\n",
    "    epoch = 1\n",
    "    frequency = -1\n",
    "    out = \"./\"\n",
    "    plot = True\n",
    "    model = L.Classifier(MLP(unit, 10))\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "    train, test = chainer.datasets.get_mnist()\n",
    "    print(train[0][0].shape)\n",
    "    print(train[0][1])    \n",
    "    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, batchsize,\n",
    "                                                 repeat=False, shuffle=False)\n",
    "    updater = training.StandardUpdater(\n",
    "        train_iter, optimizer, device=gpu)\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "    trainer.extend(extensions.Evaluator(test_iter, model, device=gpu))\n",
    "    trainer.extend(extensions.dump_graph('main/loss'))\n",
    "    frequency = epoch if frequency == -1 else max(1, frequency)\n",
    "    trainer.extend(extensions.snapshot(), trigger=(frequency, 'epoch'))\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    if plot and extensions.PlotReport.available():\n",
    "        trainer.extend(\n",
    "            extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
    "                                  'epoch', file_name='loss.png'))\n",
    "        trainer.extend(\n",
    "            extensions.PlotReport(\n",
    "                ['main/accuracy', 'validation/main/accuracy'],\n",
    "                'epoch', file_name='accuracy.png'))\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    trainer.extend(extensions.ProgressBar())\n",
    "    trainer.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4, 1)\n",
      "(150, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[variable([[5.1],\n",
       "           [3.5],\n",
       "           [1.4],\n",
       "           [0.2]]), variable([0])]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "X = X.astype(np.float32)\n",
    "X = X.reshape(150, 4, 1)\n",
    "\n",
    "Y = iris.target\n",
    "Y = Y.flatten().astype(np.int32)\n",
    "Y = Y.reshape(150, 1)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "X = Variable(X)\n",
    "Y = Variable(Y)\n",
    "\n",
    "train_data = [[x, y] for x, y in zip(X, Y)]\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in main training loop: __call__() missing 1 required positional argument: 'x'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/miyamoto/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/training/trainer.py\", line 299, in run\n",
      "    update()\n",
      "  File \"/Users/miyamoto/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/training/updater.py\", line 223, in update\n",
      "    self.update_core()\n",
      "  File \"/Users/miyamoto/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/training/updater.py\", line 238, in update_core\n",
      "    optimizer.update(loss_func, in_arrays)\n",
      "  File \"/Users/miyamoto/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/optimizer.py\", line 541, in update\n",
      "    loss = lossfun(*args, **kwds)\n",
      "  File \"/Users/miyamoto/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/links/model/classifier.py\", line 114, in __call__\n",
      "    self.y = self.predictor(*args, **kwargs)\n",
      "Will finalize trainer extensions and updater before reraising the exception.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-8672ffdc0691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 print('Will finalize trainer extensions and updater before '\n\u001b[1;32m    312\u001b[0m                       'reraising the exception.', file=sys.stderr)\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/training/updater.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \"\"\"\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/training/updater.py\u001b[0m in \u001b[0;36mupdate_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0min_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/optimizer.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, lossfun, *args, **kwds)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlossfun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0muse_cleargrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_use_cleargrads'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_cleargrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleargrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.1.11/envs/py35/lib/python3.5/site-packages/chainer/links/model/classifier.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlossfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "from chainer import datasets\n",
    "from chainer import functions as F\n",
    "from chainer import links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "class MLP(chainer.Chain):\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)\n",
    "\n",
    "batchsize = 1\n",
    "# # train, test = datasets.get_mnist()\n",
    "# train = datasets.SubDataset(train, 0, 100)\n",
    "# test = datasets.SubDataset(test, 0, 100)\n",
    "train_iter = chainer.iterators.SerialIterator(train_data, batchsize)\n",
    "test_iter = chainer.iterators.SerialIterator(train_data, batchsize,\n",
    "                                             repeat=False, shuffle=False)\n",
    "model = L.Classifier(MLP(10, 3))\n",
    "opt = chainer.optimizers.Adam()\n",
    "opt.setup(model)\n",
    "epoch = 1\n",
    "# Set up a trainer\n",
    "updater = training.StandardUpdater(train_iter, opt, device=-1)\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'), out='./')\n",
    "trainer.extend(extensions.Evaluator(test_iter, model, device=-1))\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy']))\n",
    "resume = False\n",
    "if resume:\n",
    "    chainer.serializers.load_npz(resume, trainer)\n",
    "trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
